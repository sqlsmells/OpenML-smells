UPDATE implementation set description='Implements a least median sqaured linear regression utilising the existing weka LinearRegression class to form predictions. Least squared regression functions are generated from random subsamples of the data. The least squared regression with the lowest meadian squared error is chosen as the final model.

The basis of the algorithm is 

Robust regression and outlier detection Peter J. Rousseeuw, Annick M. Leroy. c1987' where name='weka.LeastMedSq';
UPDATE implementation set description='Implementation for constructing a tree that considers K randomly  chosen attributes at each node. Performs no pruning.' where name='weka.RandomTree';
UPDATE implementation set description='Implementation for boosting a nominal class classifier using the Adaboost M1 method. Only nominal class problems can be tackled. Often dramatically improves performance, but sometimes overfits. For more information, see

Yoav Freund and Robert E. Schapire (1996). "Experiments with a new boosting algorithm".  Proc International Conference on Machine Learning, pages 148-156, Morgan Kaufmann, San Francisco.' where name='weka.AdaBoostM1';
UPDATE implementation set description='Implementation for running an arbitrary classifier on data that has been passed through an arbitrary filter. Like the classifier, the structure of the filter is based exclusively on the training data and test instances will be processed by the filter without changing their structure.' where name='weka.FilteredClassifier';
UPDATE implementation set description='This Bayes Network learning algorithm uses genetic search for finding a well scoring Bayes network structure. Genetic search works by having a population of Bayes network structures and allow them to mutate and apply cross over to get offspring. The best network structure found during the process is returned.' where name='weka.GeneticSearch';
UPDATE implementation set description='Classifier for incremental learning of large datasets by way of racing logit-boosted committees.' where name='weka.RacedIncrementalLogitBoost';
UPDATE implementation set description='Designate which class value is to be considered the "positive" class value (useful for ROC style curves).' where name='weka.ClassValuePicker';
UPDATE implementation set description='An instance filter that passes all instances through unmodified. Primarily for testing purposes.' where name='weka.Filter';
UPDATE implementation set description='Cluster data using expectation maximization' where name='weka.EM';
UPDATE implementation set description='Implementation for generating a decision tree with naive Bayes classifiers at the leaves. For more information, see

Ron Kohavi (1996). Scaling up the accuracy of naive-Bayes classifiers: a decision tree hybrid. Procedings of the Second Internaltional Conference on Knoledge Discovery and Data Mining.' where name='weka.NBTree';
UPDATE implementation set description='A data generator that produces data randomly with \'boolean\' (nominal with values {false,true}) andnumeric attributes by producing a decisionlist.' where name='weka.RDG1';
UPDATE implementation set description='An instance filter that obfuscates all strings in the data' where name='weka.Obfuscate';
UPDATE implementation set description='Takes results from a result producer and sends them to a database.' where name='weka.DatabaseResultListener';
UPDATE implementation set description='K* is an instance-based classifier, that is the class of a test instance is based upon the class of those training instances similar to it, as determined by some similarity function.  It differs from other instance-based learners in that it uses an entropy-based distance function. For more information on K*, see

John, G. Cleary and Leonard, E. Trigg (1995) "K*: An Instance- based Learner Using an Entropic Distance Measure", Proceedings of the 12th International Conference on Machine learning, pp. 108-114.' where name='weka.KStar';
UPDATE implementation set description='Implementation for combining classifiers using unweighted average of probability estimates (classification) or numeric predictions (regression).' where name='weka.Vote';
UPDATE implementation set description='Implementation for building an ensemble of randomizable base classifiers. Each base classifiers is built using a different random number seed (but based one the same data). The final prediction is a straight average of the predictions generated by the individual base classifiers.' where name='weka.RandomCommittee';
UPDATE implementation set description='This filter takes a dataset and outputs a specified fold for cross validation. If you want the folds to be stratified use the supervised version.' where name='weka.RemoveFolds';
UPDATE implementation set description='This Bayes Network learning algorithm uses cross validation to estimate classification accuracy.' where name='weka.GlobalScoreSearchAlgorithm';
UPDATE implementation set description='Takes results from a result producer and assembles them into a set of instances.' where name='weka.InstancesResultListener';
UPDATE implementation set description='Implementation for performing parameter selection by cross-validation for any classifier. For more information, see:
R. Kohavi (1995). Wrappers for Performance Enhancement and Oblivious Decision Graphs. PhD Thesis. Department of Computer Science, Stanford University.' where name='weka.CVParameterSelection';
UPDATE implementation set description='Changes the order of the classes so that the class values are no longer of in the order specified in the header. The values will be in the order specified by the user -- it could be either in ascending/descending order by the class frequency or in random order. Note that this filter currently does not change the header, only the class values of the instances, so there is not much point in using it in conjunction with the FilteredClassifier.' where name='weka.ClassOrder';
UPDATE implementation set description='This metaclassifier makes its base classifier cost-sensitive using the method specified in

Pedro Domingos (1999) "MetaCost: A general method for making classifiers cost-sensitive", Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining, pp 155-164.

This classifier should produce similar results to one created by passing the base learner to Bagging, which is in turn passed to a CostSensitiveClassifier operating on minimum expected cost. The difference is that MetaCost produces a single cost-sensitive classifier of the base learner, giving the benefits of fast classification and interpretable output (if the base learner itself is interpretable). This implementation  uses all bagging iterations when reclassifying training data (the MetaCost paper reports a marginal improvement when only those iterations containing each training instance are used in reclassifying that instance).' where name='weka.MetaCost';
UPDATE implementation set description='Implementation for generating an alternating decision tree. The basic algorithm is based on:

Freund, Y., Mason, L.: "The alternating decision tree learning algorithm". Proceeding of the Sixteenth International Conference on Machine Learning, Bled, Slovenia, (1999) 124-133.

This version currently only supports two-class problems. The number of boosting iterations needs to be manually tuned to suit the dataset and the desired complexity/accuracy tradeoff. Induction of the trees has been optimized, and heuristic search methods have been introduced to speed learning.' where name='weka.ADTree';
UPDATE implementation set description='Produces a random subsample of a dataset. The original dataset must fit entirely in memory. This filter allows you to specify the maximum "spread" between the rarest and most common class. For example, you may specify that there be at most a 2:1 difference in class frequencies. When used in batch mode, subsequent batches are NOT resampled.' where name='weka.SpreadSubsample';
UPDATE implementation set description='Split an incoming data set into cross validation folds. Separate train and test sets are produced for each of the k folds.' where name='weka.CrossValidationFoldMaker';
UPDATE implementation set description='Writes to a database' where name='weka.DatabaseSaver';
UPDATE implementation set description='Evaluate the performance of batch trained classifiers.' where name='weka.ClassifierPerformanceEvaluator';
UPDATE implementation set description='A filter that removes a given percentage of a dataset.' where name='weka.RemovePercentage';
UPDATE implementation set description='Filters instances according to the value of an attribute.' where name='weka.RemoveWithValues';
UPDATE implementation set description='Implementation for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).' where name='weka.Classifier';
UPDATE implementation set description='A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy.' where name='weka.MultiClassClassifier';
UPDATE implementation set description='Designate an incoming data set as a training set.' where name='weka.TrainingSetMaker';
UPDATE implementation set description='ReliefFAttributeEval :

Evaluates the worth of an attribute by repeatedly sampling an instance and considering the value of the given attribute for the nearest instance of the same and different class. Can operate on both discrete and continuous class data.
' where name='weka.ReliefFAttributeEval';
UPDATE implementation set description='A filter that adds a new nominal attribute representing the cluster assigned to each instance by the specified clustering algorithm.' where name='weka.AddCluster';
UPDATE implementation set description='Implementation for building and using a 0-R classifier. Predicts the mean (for a numeric class) or the mode (for a nominal class).' where name='weka.ZeroR';
UPDATE implementation set description='An instance filter that normalize instances considering only numeric attributes and ignoring class index.' where name='weka.Normalize';
UPDATE implementation set description='ChiSquaredAttributeEval :

Evaluates the worth of an attribute by computing the value of the chi-squared statistic with respect to the class.
' where name='weka.ChiSquaredAttributeEval';
UPDATE implementation set description='Graphically visualize trees or graphs produced by classifiers/clusterers.' where name='weka.GraphViewer';
UPDATE implementation set description='Implementation for boosting a classifier using the MultiBoosting method.

MultiBoosting is an extension to the highly successful AdaBoost technique for forming decision committees. MultiBoosting can be viewed as combining AdaBoost with wagging. It is able to harness both AdaBoost\'s high bias and variance reduction with wagging\'s superior variance reduction. Using C4.5 as the base learning algorithm, Multi-boosting is demonstrated to produce decision committees with lower error than either AdaBoost or wagging significantly more often than the reverse over a large representative cross-section of UCI data sets. It offers the further advantage over AdaBoost of suiting parallel execution. For more information, see

Geoffrey I. Webb (2000). "MultiBoosting: A Technique for Combining Boosting and Wagging".  Machine Learning, 40(2): 159-196, Kluwer Academic Publishers, Boston' where name='weka.MultiBoostAB';
UPDATE implementation set description='Reads a source that is in arff (attribute relation file format) format. ' where name='weka.ArffLoader';
UPDATE implementation set description='Implementation of the voted perceptron algorithm by Freund and Schapire. Globally replaces all missing values, and transforms nominal attributes into binary ones. For more information, see:

Y. Freund and R. E. Schapire (1998). Large margin classification using the perceptron algorithm.  Proc. 11th Annu. Conf. on Comput. Learning Theory, pp. 209-217, ACM Press, New York, NY.' where name='weka.VotedPerceptron';
UPDATE implementation set description='Implementation implementing a HyperPipe classifier. For each category a HyperPipe is constructed that contains all points of that category (essentially records the attribute bounds observed for each category). Test instances are classified according to the category that "most contains the instance". Does not handle numeric class, or missing values in test cases. Extremely simple algorithm, but has the advantage of being extremely fast, and works quite well when you have "smegloads" of attributes.' where name='weka.HyperPipes';
UPDATE implementation set description='Implementation for performing locally weighted learning. Can do classification (e.g. using naive Bayes) or regression (e.g. using linear regression). The base learner needs to implement WeightedInstancesHandler. For more info, see

Eibe Frank, Mark Hall, and Bernhard Pfahringer (2003). "Locally Weighted Naive Bayes". Conference on Uncertainty in AI.

Atkeson, C., A. Moore, and S. Schaal (1996) "Locally weighted learning" AI Reviews.' where name='weka.LWL';
UPDATE implementation set description='Randomly shuffles the order of instances passed through it. The random number generator is reset with the seed value whenever a new set of instances is passed in.' where name='weka.Randomize';
UPDATE implementation set description='GainRatioAttributeEval :

Evaluates the worth of an attribute by measuring the gain ratio with respect to the class.

GainR(Class, Attribute) = (H(Class) - H(Class | Attribute)) / H(Attribute).
' where name='weka.GainRatioAttributeEval';
UPDATE implementation set description='InfoGainAttributeEval :

Evaluates the worth of an attribute by measuring the information gain with respect to the class.

InfoGain(Class,Attribute) = H(Class) - H(Class | Attribute).
' where name='weka.InfoGainAttributeEval';
UPDATE implementation set description='Writes to a destination that is in arff (attribute relation file format) format. ' where name='weka.Saver';
UPDATE implementation set description='Cluster data using the k means algorithm' where name='weka.SimpleKMeans';
UPDATE implementation set description='Ranker : 

Ranks attributes by their individual evaluations. Use in conjunction with attribute evaluators (ReliefF, GainRatio, Entropy etc).
' where name='weka.Ranker';
UPDATE implementation set description=' Meta classifier that enhances the performance of a regression base classifier. Each iteration fits a model to the residuals left by the classifier on the previous iteration. Prediction is accomplished by adding the predictions of each classifier. Reducing the shrinkage (learning rate) parameter helps prevent overfitting and has a smoothing effect but increases the learning time.  For more information see: Friedman, J.H. (1999). Stochastic Gradient Boosting. Technical Report Stanford University. http://www-stat.stanford.edu/~jhf/ftp/stobst.ps.' where name='weka.AdditiveRegression';
UPDATE implementation set description='Generates a decision list for regression problems using separate-and-conquer. In each iteration it builds a model tree using M5 and makes the "best" leaf into a rule. Reference:

M. Hall, G. Holmes, E. Frank (1999).  "Generating Rule Sets from Model Trees". Proceedings of the Twelfth Australian Joint Conference on Artificial Intelligence, Sydney, Australia. Springer-Verlag, pp. 1-12.' where name='weka.M5Rules';
UPDATE implementation set description='Dimensionality of training and test data is reduced by attribute selection before being passed on to a classifier.' where name='weka.AttributeSelectedClassifier';
UPDATE implementation set description='An Estimator is an algorithm for finding the conditional probability tables of the Bayes Network.' where name='weka.MultiNomialBMAEstimator';
UPDATE implementation set description='Accepts batch or incremental classifier events and produces a new data set with classifier predictions appended.' where name='weka.PredictionAppender';
UPDATE implementation set description=' A SplitEvaluator that produces results for a classification scheme on a nominal class attribute.' where name='weka.ClassifierSplitEvaluator';
UPDATE implementation set description='This Bayes Network learning algorithm determines the maximum weight spanning tree  and returns a Naive Bayes network augmented with a tree.' where name='weka.TAN';
UPDATE implementation set description='A SplitEvaluator that produces results for a classification scheme on a numeric class attribute.' where name='weka.RegressionSplitEvaluator';
UPDATE implementation set description='This Bayes Network learning algorithm uses tabu search for finding a well scoring Bayes network structure. Tabu search is hill climbing till an optimum is reached. The following step is the least worst possible step. The last X steps are kept in a list and none of the steps in this so called tabu list is considered in taking the next step. The best network found in this traversal is returned.' where name='weka.TabuSearch';
UPDATE implementation set description='Reduces the dimensionality of the data by projecting it onto a lower dimensional subspace using a random matrix with columns of unit length (i.e. It will reduce the number of attributes in the data while preserving much of its variation like PCA, but at a much less computational cost).
It first applies the  NominalToBinary filter to convert all attributes to numeric before reducing the dimension. It preserves the class attribute.' where name='weka.RandomProjection';
UPDATE implementation set description='The implementation of a RIpple-DOwn Rule learner. It generates a default rule first and then the exceptions for the default rule with the least (weighted) error rate.  Then it generates the "best" exceptions for each exception and iterates until pure.  Thus it performs a tree-like expansion of exceptions.The exceptions are a set of rules that predict classes other than the default. IREP is used to generate the exceptions.' where name='weka.Ridor';
UPDATE implementation set description='Converts a string attribute (i.e. unspecified number of values) to nominal (i.e. set number of values). You should ensure that all string values that will appear are represented in the first batch of the data.' where name='weka.StringToNominal';
UPDATE implementation set description='Bayes Network learning using various search algorithms and quality measures.' where name='weka.BIFReader';
UPDATE implementation set description='Split an incoming data set into separate train and test sets.' where name='weka.TrainTestSplitMaker';
UPDATE implementation set description='Writes to a destination that is in arff (attribute relation file format) format. ' where name='weka.ArffSaver';
UPDATE implementation set description='A filter that creates a new dataset with a boolean attribute replacing a nominal attribute.  In the new dataset, a value of 1 is assigned to an instance that exhibits a particular range of attribute values, a 0 to an instance that doesn\'t. The boolean attribute is coded as numeric by default.' where name='weka.MakeIndicator';
UPDATE implementation set description='Finds association rules sorted by predictive accuracy.' where name='weka.PredictiveApriori';
UPDATE implementation set description='A regression scheme that employs any classifier on a copy of the data that has the class attribute (equal-width) discretized. The predicted value is the expected value of the mean class value for each discretized interval (based on the predicted probabilities for each interval).' where name='weka.RegressionByDiscretization';
UPDATE implementation set description='This Bayes Network learning algorithm uses conditional independence tests to find a skeleton, finds V-nodes and applies a set of rules to find the directions of the remaining arrows.' where name='weka.ICSSearchAlgorithm';
UPDATE implementation set description='GreedyStepwise :

Performs a greedy forward or backward search through the space of attribute subsets. May start with no/all attributes or from an arbitrary point in the space. Stops when the addition/deletion of any remaining attributes results in a decrease in evaluation. Can also produce a ranked list of attributes by traversing the space from one side to the other and recording the order that attributes are selected.
' where name='weka.GreedyStepwise';
UPDATE implementation set description='Changes the format used by a date attribute.' where name='weka.ChangeDateFormat';
UPDATE implementation set description='An instance filter that assumes instances form time-series data and replaces attribute values in the current instance with the equivalent attribute attribute values of some previous (or future) instance. For instances where the desired value is unknown either the instance may  be dropped, or missing values used. Skips the class attribute if it is set.' where name='weka.TimeSeriesTranslate';
UPDATE implementation set description='Implements Winnow and Balanced Winnow algorithms by Littlestone. For more information, see

N. Littlestone (1988). "Learning quickly when irrelevant attributes are abound: A new linear threshold algorithm". Machine Learning 2, pp. 285-318.

and

N. Littlestone (1989). "Mistake bounds and logarithmic  linear-threshold learning algorithms". Technical report UCSC-CRL-89-11, University of California, Santa Cruz.

Does classification for problems with nominal attributes (which it converts into binary attributes).' where name='weka.Winnow';
UPDATE implementation set description='Races the cross validation error of competing attribute subsets. Use in conjuction with a ClassifierSubsetEval. RaceSearch has four modes:

forward selection races all single attribute additions to a base set (initially  no attributes), selects the winner to become the new base set and then iterates until there is no improvement over the base set. 

Backward elimination is similar but the initial base set has all attributes included and races all single attribute deletions. 

Schemata search is a bit different. Each iteration a series of races are run in parallel. Each race in a set determines whether a particular attribute should be included or not---ie the race is between the attribute being "in" or "out". The other attributes for this race are included or excluded randomly at each point in the evaluation. As soon as one race has a clear winner (ie it has been decided whether a particular attribute should be inor not) then the next set of races begins, using the result of the winning race from the previous iteration as new base set.

Rank race first ranks the attributes using an attribute evaluator and then races the ranking. The race includes no attributes, the top ranked attribute, the top two attributes, the top three attributes, etc.

It is also possible to generate a raked list of attributes through the forward racing process. If generateRanking is set to true then a complete forward race will be run---that is, racing continues until all attributes have been selected. The order that they are added in determines a complete ranking of all the attributes.

Racing uses paired and unpaired t-tests on cross-validation errors of competing subsets. When there is a significant difference between the means of the errors of two competing subsets then the poorer of the two can be eliminated from the race. Similarly, if there is no significant difference between the mean errors of two competing subsets and they are within some threshold of each other, then one can be eliminated from the race. ' where name='weka.RaceSearch';
UPDATE implementation set description='This Bayes Network learning algorithm repeatedly uses hill climbing starting with a randomly generated network structure and return the best structure of the various runs.' where name='weka.RepeatedHillClimber';
UPDATE implementation set description='Implementation for constructing an unpruned decision tree based on the ID3 algorithm. Can only deal with nominal attributes. No missing values allowed. Empty leaves may result in unclassified instances. For more information see: 

 R. Quinlan (1986). "Induction of decision trees". Machine Learning. Vol.1, No.1, pp. 81-106' where name='weka.Id3';
UPDATE implementation set description='Implementation for building and using a Complement class Naive Bayes classifier. For more information see, 
ICML-2003 "Tackling the poor assumptions of Naive Bayes Text Classifiers" 
P.S.: TF, IDF and length normalization transforms, as described in the paper, can be performed through weka.filters.unsupervised.StringToWordVector.' where name='weka.ComplementNaiveBayes';
UPDATE implementation set description='Lazy Bayesian Rules Classifier. The naive Bayesian classifier provides a simple and effective approach to classifier learning, but its attribute independence assumption is often violated in the real world. Lazy Bayesian Rules selectively relaxes the independence assumption, achieving lower error rates over a range of learning tasks.  LBR defers processing to classification time, making it a highly efficient and accurate classification algorithm when small numbers of objects are to be classified.' where name='weka.LBR';
UPDATE implementation set description='Classification by voting feature intervals. Intervals are constucted around each class for each attribute (basically discretization). Class counts are recorded for each interval on each attribute. Classification is by voting. For more info see Demiroz, G. and Guvenir, A. (1997) "Classification by voting feature intervals", ECML-97.

Have added a simple attribute weighting scheme. Higher weight is assigned to more confident intervals, where confidence is a function of entropy:
weight (att_i) = (entropy of class distrib att_i / max uncertainty)^-bias' where name='weka.VFI';
UPDATE implementation set description='Converts all numeric attributes into binary attributes (apart from the class attribute, if set): if the value of the numeric attribute is exactly zero, the value of the new attribute will be zero. If the value of the numeric attribute is missing, the value of the new attribute will be missing. Otherwise, the value of the new attribute will be one. The new attributes will nominal.' where name='weka.NumericToBinary';
UPDATE implementation set description='An Estimator is an algorithm for finding the conditional probability tables of the Bayes Network.' where name='weka.BayesNetEstimator';
UPDATE implementation set description='Converts String attributes into a set of attributes representing word occurrence information from the text contained in the strings. The set of words (attributes) is determined by the first batch filtered (typically training data).' where name='weka.StringToWordVector';
UPDATE implementation set description='A metaclassifier that selecting a mid-point threshold on the probability output by a Classifier. The midpoint threshold is set so that a given performance measure is optimized. Currently this is the F-measure. Performance is measured either on the training data, a hold-out set or using cross-validation. In addition, the probabilities returned by the base learner can have their range expanded so that the output probabilities will reside between 0 and 1 (this is useful if the scheme normally produces probabilities in a very narrow range).' where name='weka.ThresholdSelector';
UPDATE implementation set description='Implements Alex Smola and Bernhard Scholkopf\'s sequential minimal optimization algorithm for training a support vector regression model. This implementation globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes by default. (Note that the coefficients in the output are based on the normalized/standardized data, not the original data.) For more information on the SMO algorithm, see

Alex J. Smola, Bernhard Scholkopf (1998). "A Tutorial on Support Vector Regression".  NeuroCOLT2 Technical Report Series - NC2-TR-1998-030.

S.K. Shevade, S.S. Keerthi, C. Bhattacharyya, K.R.K. Murthy,  "Improvements to SMO Algorithm for SVM Regression".  Technical Report CD-99-16, Control Division Dept of Mechanical and Production Engineering, National University of Singapore. ' where name='weka.SMOreg';
UPDATE implementation set description='Nearest-neighbour classifier. Uses normalized Euclidean distance to find the training instance closest to the given test instance, and predicts the same class as this training instance. If multiple instances have the same (smallest) distance to the test instance, the first one found is used.  For more information, see 

Aha, D., and D. Kibler (1991) "Instance-based learning algorithms", Machine Learning, vol.6, pp. 37-66.' where name='weka.IB1';
UPDATE implementation set description='Implementation for performing additive logistic regression. This class performs classification using a regression scheme as the base learner, and can handle multi-class problems.  For more information, see

Friedman, J., T. Hastie and R. Tibshirani (1998) "Additive Logistic Regression: a Statistical View of Boosting". Technical report. Stanford University.

Can do efficient internal cross-validation to determine appropriate number of iterations.' where name='weka.LogitBoost';
UPDATE implementation set description='This Bayes Network learning algorithm uses a hill climbing algorithm restricted by an order on the variables' where name='weka.K2';
UPDATE implementation set description='Visualize incoming data/training/test sets in a scatter plot matrix.' where name='weka.ScatterPlotMatrix';
UPDATE implementation set description='Designate which column is to be considered the class column in incoming data.' where name='weka.ClassAssigner';
UPDATE implementation set description='Implementation for building and using a multinomial Naive Bayes classifier. For more information see,

Andrew Mccallum, Kamal Nigam (1998) A Comparison of Event Models for Naive Bayes Text Classification' where name='weka.NaiveBayesMultinomial';
UPDATE implementation set description='Examines a database and extracts out the results produced by the specified ResultProducer and submits them to the specified ResultListener. If a result needs to be generated, the ResultProducer is used to obtain the result.' where name='weka.DatabaseResultProducer';
UPDATE implementation set description='Serializes the instances to a file with extension bsi.' where name='weka.SerializedInstancesSaver';
UPDATE implementation set description='ConsistencySubsetEval :

Evaluates the worth of a subset of attributes by the level of consistency in the class values when the training instances are projected onto the subset of attributes. 

Consistency of any subset can never be lower than that of the full set of attributes, hence the usual practice is to use this subset evaluator in conjunction with a Random or Exhaustive search which looks for the smallest subset with consistency equal to that of the full set of attributes.
' where name='weka.ConsistencySubsetEval';
UPDATE implementation set description='Implementation for building and using a decision stump. Usually used in conjunction with a boosting algorithm. Does regression (based on mean-squared error) or classification (based on entropy). Missing is treated as a separate value.' where name='weka.DecisionStump';
UPDATE implementation set description='An instance filter that creates a new attribute by applying a mathematical expression to existing attributes. The expression can contain attribute references and numeric constants. Supported opperators are :  +, -, *, /, ^, log, abs, cos, exp, sqrt, floor, ceil, rint, tan, sin, (, ). Attributes are specified by prefixing with \'a\', eg. a7 is attribute number 7 (starting from 1). Example expression : a1^2*a5/log(a7*4.0).' where name='weka.AddExpression';
UPDATE implementation set description='Discretizes numeric attributes using equal frequency binning, where the number of bins is equal to the square root of the number of non-missing values.' where name='weka.PKIDiscretize';
UPDATE implementation set description='An instance filter that removes a range of attributes from the dataset.' where name='weka.Remove';
UPDATE implementation set description='OneRAttributeEval :

Evaluates the worth of an attribute by using the OneR classifier.
' where name='weka.OneRAttributeEval';
UPDATE implementation set description='WrapperSubsetEval:

Evaluates attribute sets by using a learning scheme. Cross validation is used to estimate the accuracy of the learning scheme for a set of attributes.
' where name='weka.WrapperSubsetEval';
UPDATE implementation set description='Cluster data using the FarthestFirst algorithm' where name='weka.FarthestFirst';
UPDATE implementation set description='This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. 

The algorithm is briefly described as follows: 

Initialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: 

1. Building stage:
Repeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. 

1.1. Grow phase:
Grow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).

1.2. Prune phase:
Incrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it\'s actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it\'s 0.5).

2. Optimization stage:
 after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. 
3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. 
ENDDO

Note that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen\'s original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.

Details please see "Fast Effective Rule Induction", William W. Cohen, \'Machine Learning: Proceedings of the Twelfth International Conference\'(ML95). 

PS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data "ab+bcd+defg" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn\'t consider memory consumption optimization in this implementation.

' where name='weka.JRip';
UPDATE implementation set description='Performs a random train and test using a supplied evaluator.' where name='weka.RandomSplitResultProducer';
UPDATE implementation set description='A filter that removes a given range of instances of a dataset.' where name='weka.RemoveRange';
UPDATE implementation set description='K-nearest neighbours classifier. Normalizes attributes by default. Can select appropriate value of K based on cross-validation. Can also do distance weighting. For more information, see

Aha, D., and D. Kibler (1991) "Instance-based learning algorithms", Machine Learning, vol.6, pp. 37-66.' where name='weka.IBk';
UPDATE implementation set description='An Estimator is an algorithm for finding the conditional probability tables of the Bayes Network.' where name='weka.SimpleEstimator';
UPDATE implementation set description='Implementation for building and using a simple decision table majority classifier. For more information see: 

Kohavi R. (1995). "The Power of Decision Tables." In Proc European Conference on Machine Learning.' where name='weka.DecisionTable';
UPDATE implementation set description='A filter that uses a density-based clusterer to generate cluster membership values; filtered instances are composed of these values plus the class attribute (if set in the input data). If a (nominal) class attribute is set, the clusterer is run separately for each class. The class attribute (if set) and any user-specified attributes are ignored during the clustering operation' where name='weka.ClusterMembership';
UPDATE implementation set description='Produces a random subsample of a dataset using sampling withreplacement. The original dataset must fit entirely in memory. The number of instances in the generated dataset may be specified.' where name='weka.Resample';
UPDATE implementation set description='Takes the results from a ResultProducer and submits the average to the result listener. Normally used with a CrossValidationResultProducer to perform n x m fold cross validation.' where name='weka.AveragingResultProducer';
UPDATE implementation set description='Bayes Network learning using various search algorithms and quality measures.' where name='weka.BayesNetGenerator';
UPDATE implementation set description='Evaluate the performance of incrementally trained classifiers.' where name='weka.IncrementalClassifierEvaluator';
UPDATE implementation set description='Cluster data using expectation maximization' where name='weka.Clusterer';
UPDATE implementation set description='Designate an incoming data set as a test set.' where name='weka.TestSetMaker';
UPDATE implementation set description='This neural network uses backpropagation to train.' where name='weka.MultilayerPerceptron';
UPDATE implementation set description='Replaces all missing values for nominal and numeric attributes in a dataset with the modes and means from the training data.' where name='weka.ReplaceMissingValues';
UPDATE implementation set description='Finds rules according to confirmation measure.' where name='weka.Tertius';
UPDATE implementation set description='This class implements a single conjunctive rule learner that can predict for numeric and nominal class labels.

A rule consists of antecedents "AND"ed together and the consequent (class value) for the classification/regression.  In this case, the consequent is the distribution of the available classes (or mean for a numeric value) in the dataset. If the test instance is not covered by this rule, then it\'s predicted using the default class distributions/value of the data not covered by the rule in the training data.This learner selects an antecedent by computing the Information Gain of each antecendent and prunes the generated rule using Reduced Error Prunning (REP) or simple pre-pruning based on the number of antecedents.

For classification, the Information of one antecedent is the weighted average of the entropies of both the data covered and not covered by the rule.
For regression, the Information is the weighted average of the mean-squared errors of both the data covered and not covered by the rule.

In pruning, weighted average of the accuracy rates on the pruning data is used for classification while the weighted average of the mean-squared errors on the pruning data is used for regression.

' where name='weka.ConjunctiveRule';
UPDATE implementation set description='Converts all nominal attributes into binary numeric attributes. An attribute with k values is transformed into k binary attributes if the class is nominal (using the one-attribute-per-value approach). Binary attributes are left binary.If the class is numeric, you might want to use the supervised version of this filter.' where name='weka.NominalToBinary';
UPDATE implementation set description='AODE achieves highly accurate classification by averaging over all of a small space of alternative naive-Bayes-like models that have weaker (and hence less detrimental) independence assumptions than naive Bayes. The resulting algorithm is computationally efficient while delivering highly accurate classification on many learning tasks.

For more information, see

G. Webb, J. Boughton & Z. Wang (2004). Not So Naive Bayes. To be published in Machine Learning. G. Webb, J. Boughton & Z. Wang (2002). <i>Averaged One-Dependence Estimators: Preliminary Results. AI2002 Data Mining Workshop, Canberra.' where name='weka.AODE';
UPDATE implementation set description='A supervised attribute filter that can be used to select attributes. It is very flexible and allows various search and evaluation methods to be combined.' where name='weka.AttributeSelection';
UPDATE implementation set description='RandomSearch : 

Performs a Random search in the space of attribute subsets. If no start set is supplied, Random search starts from a random point and reports the best subset found. If a start set is supplied, Random searches randomly for subsets that are as good or better than the start point with the same or or fewer attributes. Using RandomSearch in conjunction with a start set containing all attributes equates to the LVF algorithm of Liu and Setiono (ICML-96).
' where name='weka.RandomSearch';
UPDATE implementation set description='An instance filter that passes all instances through unmodified. Primarily for testing purposes.' where name='weka.AllFilter';
UPDATE implementation set description='An instance filter that converts all incoming instances into sparse format.' where name='weka.NonSparseToSparse';
UPDATE implementation set description='An instance filter that changes a percentage of a given attributes values. The attribute must be nominal. Missing value can be treated as value itself.' where name='weka.AddNoise';
UPDATE implementation set description='Implementation for building pace regression linear models and using them for prediction. 

Under regularity conditions, pace regression is provably optimal when the number of coefficients tends to infinity. It consists of a group of estimators that are either overall optimal or optimal under certain conditions.

The current work of the pace regression theory, and therefore also this implementation, do not handle: 

- missing values 
- non-binary nominal attributes 
- the case that n - k is small where n is the number of instances and k is the number of coefficients (the threshold used in this implmentation is 20)

For more information see:

Wang, Y. (2000). A new approach to fitting linear models in high dimensional spaces. PhD Thesis. Department of Computer Science, University of Waikato, New Zealand. 

Wang, Y. and Witten, I. H. (2002). Modeling for optimal probability prediction. Proceedings of ICML\'2002. Sydney.' where name='weka.PaceRegression';
UPDATE implementation set description='Implementation for generating a PART decision list. Uses separate-and-conquer. Builds a partial C4.5 decision tree in each iteration and makes the "best" leaf into a rule. For more information, see:

Eibe Frank and Ian H. Witten (1998). "Generating Accurate Rule Sets Without Global Optimization."In Shavlik, J., ed., Machine Learning: Proceedings of the Fifteenth International Conference, Morgan Kaufmann Publishers.' where name='weka.PART';
UPDATE implementation set description='This filter takes a dataset and outputs a specified fold for cross validation. If you do not want the folds to be stratified use the unsupervised version.' where name='weka.StratifiedRemoveFolds';
UPDATE implementation set description='This Bayes Network learning algorithm uses the general purpose search method of simulated annealing to find a well scoring network structure.' where name='weka.SimulatedAnnealing';
UPDATE implementation set description='An instance filter that adds a new attribute to the dataset. The new attribute will contain all missing values.' where name='weka.Add';
UPDATE implementation set description='ExhaustiveSearch : 

Performs an exhaustive search through the space of attribute subsets starting from the empty set of attrubutes. Reports the best subset found.' where name='weka.ExhaustiveSearch';
UPDATE implementation set description='An Estimator is an algorithm for finding the conditional probability tables of the Bayes Network.' where name='weka.BMAEstimator';
UPDATE implementation set description='This Bayes Network learning algorithm uses a hill climbing algorithm adding, deleting and reversing arcs. The search is not restricted by an order on the variables (unlike K2). The difference with B and B2 is that this hill climber also considers arrows part of the naive Bayes structure for deletion.' where name='weka.HillClimber';
UPDATE implementation set description='Fast decision tree learner. Builds a decision/regression tree using information gain/variance and prunes it using reduced-error pruning (with backfitting).  Only sorts values for numeric attributes once. Missing values are dealt with by splitting the corresponding instances into pieces (i.e. as in C4.5).' where name='weka.REPTree';
UPDATE implementation set description='Implementation for building and using a 1R classifier; in other words, uses the minimum-error attribute for prediction, discretizing numeric attributes. For more information, see

:R.C. Holte (1993). "Very simple classification rules perform well on most commonly used datasets". Machine Learning, Vol. 11, pp. 63-91.' where name='weka.OneR';
UPDATE implementation set description='Classifier for building \'logistic model trees\', which are classification trees with logistic regression functions at the leaves. The algorithm can deal with binary and multi-class target variables, numeric and nominal attributes and missing values. For more information see: N.Landwehr, M.Hall, E. Frank \'Logistic Model Trees\' (ECML 2003).' where name='weka.LMT';
UPDATE implementation set description='Combines several classifiers using the stacking method. Can do classification or regression. For more information, see

David H. Wolpert (1992). "Stacked generalization". Neural Networks, 5:241-259, Pergamon Press.' where name='weka.Stacking';
UPDATE implementation set description='Implementation for building and using a multinomial logistic regression model with a ridge estimator.

There are some modifications, however, compared to the paper of leCessie and van Houwelingen(1992): 

If there are k classes for n instances with m attributes, the parameter matrix B to be calculated will be an m*(k-1) matrix.

The probability for class j with the exception of the last class is

Pj(Xi) = exp(XiBj)/((sum[j=1..(k-1)]exp(Xi*Bj))+1) 

The last class has probability

1-(sum[j=1..(k-1)]Pj(Xi)) 
	= 1/((sum[j=1..(k-1)]exp(Xi*Bj))+1)

The (negative) multinomial log-likelihood is thus: 

L = -sum[i=1..n]{
	sum[j=1..(k-1)](Yij * ln(Pj(Xi)))
	+(1 - (sum[j=1..(k-1)]Yij)) 
	* ln(1 - sum[j=1..(k-1)]Pj(Xi))
	} + ridge * (B^2)

In order to find the matrix B for which L is minimised, a Quasi-Newton Method is used to search for the optimized values of the m*(k-1) variables.  Note that before we use the optimization procedure, we \'squeeze\' the matrix B into a m*(k-1) vector.  For details of the optimization procedure, please check weka.core.Optimization class.

Although original Logistic Regression does not deal with instance weights, we modify the algorithm a little bit to handle the instance weights.

For more information see:

le Cessie, S. and van Houwelingen, J.C. (1992). Ridge Estimators in Logistic Regression.  Applied Statistics, Vol. 41, No. 1, pp. 191-201. 

Note: Missing values are replaced using a ReplaceMissingValuesFilter, and nominal attributes are transformed into numeric attributes using a NominalToBinaryFilter.' where name='weka.Logistic';
UPDATE implementation set description='An instance filter that converts all incoming sparse instances into non-sparse format.' where name='weka.SparseToNonSparse';
UPDATE implementation set description='Visualize incremental classifier performance as a scrolling plot.' where name='weka.StripChart';
UPDATE implementation set description=' Meta classifier that allows standard classification algorithms to be applied to ordinal class problems.  For more information see: Frank, E. and Hall, M. (in press). A simple approach to ordinal prediction. 12th European Conference on Machine Learning. Freiburg, Germany.' where name='weka.OrdinalClassClassifier';
UPDATE implementation set description='Plot summary bar charts for incoming data/training/test sets.' where name='weka.AttributeSummarizer';
UPDATE implementation set description='A filter that removes instances which are incorrectly classified. Useful for removing outliers.' where name='weka.RemoveMisclassified';
UPDATE implementation set description='Reads a source that is in comma separated or tab separated format. Assumes that the first row in the file determines the number of and names of the attributes.' where name='weka.CSVLoader';
UPDATE implementation set description=' SplitEvaluator that produces results for a classification scheme on a nominal class attribute, including weighted misclassification costs.' where name='weka.CostSensitiveClassifierSplitEvaluator';
UPDATE implementation set description='Implements Grading. The base classifiers are "graded". For more information, see

Seewald A.K., Fuernkranz J. (2001): An Evaluation of Grading Classifiers, in Hoffmann F. et al. (eds.), Advances in Intelligent Data Analysis, 4th International Conference, IDA 2001, Proceedings, Springer, Berlin/Heidelberg/New York/Tokyo, pp.115-124, 2001' where name='weka.Grading';
UPDATE implementation set description='Interactively classify through visual means. You are Presented with a scatter graph of the data against two user selectable attributes, as well as a view of the decision tree. You can create binary splits by creating polygons around data plotted on the scatter graph, as well as by allowing another classifier to take over at points in the decision tree should you see fit.' where name='weka.UserClassifier';
UPDATE implementation set description='This instance filter takes a range of N numeric attributes and replaces them with N-1 numeric attributes, the values of which are the difference between consecutive attribute values from the original instance. eg: 

Original attribute values

   0.1, 0.2, 0.3, 0.1, 0.3

New attribute values

   0.1, 0.1, -0.2, 0.2

The range of attributes used is taken in numeric order. That is, a range spec of 7-11,3-5 will use the attribute ordering 3,4,5,7,8,9,10,11 for the differences, NOT 7,8,9,10,11,3,4,5.' where name='weka.FirstOrder';
UPDATE implementation set description='Swaps two values of a nominal attribute.' where name='weka.SwapValues';
UPDATE implementation set description='Removes constant attributes, along with nominal attributes that vary too much.' where name='weka.RemoveUseless';
UPDATE implementation set description='General purpose text display.' where name='weka.TextViewer';
UPDATE implementation set description='Implementation for constructing a forest of random trees. For more information see: 

Leo Breiman. "Random Forests". Machine Learning 45 (1):5-32, October 2001.' where name='weka.RandomForest';
UPDATE implementation set description='Implementation for building and using a PRISM rule set for classification. Can only deal with nominal attributes. Can\'t deal with missing values. Doesn\'t do any pruning. For more information, see 

J. Cendrowska (1987). "PRISM: An algorithm for inducing modular rules". International Journal of Man-Machine Studies. Vol.27, No.4, pp.349-370.' where name='weka.Prism';
UPDATE implementation set description='Implementation for a Naive Bayes classifier using estimator classes. This is the updateable version of NaiveBayes.This classifier will use a default precision of 0.1 for numeric attributes when buildClassifier is called with zero training instances.

For more information on Naive Bayes classifiers, see

George H. John and Pat Langley (1995). Estimating Continuous Distributions in Bayesian Classifiers. Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. pp. 338-345. Morgan Kaufmann, San Mateo.

' where name='weka.NaiveBayesUpdateable';
UPDATE implementation set description='Implementation that implements a normalized Gaussian radial basisbasis function network. It uses the k-means clustering algorithm to provide the basis functions and learns either a logistic regression (discrete class problems) or linear regression (numeric class problems) on top of that. Symmetric multivariate Gaussians are fit to the data from each cluster. If the class is nominal it uses the given number of clusters per class.It standardizes all numeric attributes to zero mean and unit variance.' where name='weka.RBFNetwork';
UPDATE implementation set description='Transforms numeric attributes using a given transformation method.' where name='weka.NumericTransform';
UPDATE implementation set description='Writes to a destination that is in csv format' where name='weka.CSVSaver';
UPDATE implementation set description='Reads a file that is C45 format. Can take a filestem or filestem with .names or .data appended. Assumes that path/<filestem>.names and path/<filestem>.data exist and contain the names and data respectively.' where name='weka.C45Loader';
UPDATE implementation set description='Visualize performance charts (such as ROC).' where name='weka.ModelPerformanceChart';
UPDATE implementation set description='An instance filter that discretizes a range of numeric attributes in the dataset into nominal attributes. Discretization is by simple binning. Skips the class attribute if set.' where name='weka.Discretize';
UPDATE implementation set description='RankSearch : 

Uses an attribute/subset evaluator to rank all attributes. If a subset evaluator is specified, then a forward selection search is used to generate a ranked list. From the ranked list of attributes, subsets of increasing size are evaluated, ie. The best attribute, the best attribute plus the next best attribute, etc.... The best attribute set is reported. RankSearch is linear in the number of attributes if a simple attribute evaluator is used such as GainRatioAttributeEval.
' where name='weka.RankSearch';
UPDATE implementation set description='Standardizes all numeric attributes in the given dataset to have zero mean and unit variance (apart from the class attribute, if set).' where name='weka.Standardize';
UPDATE implementation set description='A metaclassifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: reweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.' where name='weka.CostSensitiveClassifier';
UPDATE implementation set description='BestFirst:

Searches the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done. Best first may start with the empty set of attributes and search forward, or start with the full set of attributes and search backward, or start at any point and search in both directions (by considering all possible single attribute additions and deletions at a given point).
' where name='weka.BestFirst';
UPDATE implementation set description='Evaluates attribute subsets on training data or a seperate hold out testing set' where name='weka.ClassifierSubsetEval';
UPDATE implementation set description='Evaluate the performance of batch trained clusterers.' where name='weka.ClustererPerformanceEvaluator';
UPDATE implementation set description='Bayes Network learning using various search algorithms and quality measures.' where name='weka.BayesNet';
UPDATE implementation set description='Finds association rules.' where name='weka.Apriori';
UPDATE implementation set description='Learns a simple linear regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes.' where name='weka.SimpleLinearRegression';
UPDATE implementation set description='Performs a principal components analysis and transformation of the data. Use in conjunction with a Ranker search. Dimensionality reduction is accomplished by choosing enough eigenvectors to account for some percentage of the variance in the original data---default 0.95 (95%). Attribute noise can be filtered by transforming to the PC space, eliminating some of the worst eigenvectors, and then transforming back to the original space.' where name='weka.PrincipalComponents';
UPDATE implementation set description='Classifier for building linear logistic regression models. LogitBoost with simple regression functions as base learners is used for fitting the logistic models. The optimal number of LogitBoost iterations to perform is cross-validated, which leads to automatic attribute selection. For more information see: N.Landwehr, M.Hall, E. Frank \'Logistic Model Trees\' (ECML 2003).' where name='weka.SimpleLogistic';
UPDATE implementation set description='An instance filter that assumes instances form time-series data and replaces attribute values in the current instance with the difference between the current value and the equivalent attribute attribute value of some previous (or future) instance. For instances where the time-shifted value is unknown either the instance may be dropped, or missing values used. Skips the class attribute if it is set.' where name='weka.TimeSeriesDelta';
UPDATE implementation set description='Implementation for selecting a classifier from among several using cross validation on the training data or the performance on the training data. Performance is measured based on percent correct (classification) or mean-squared error (regression).' where name='weka.MultiScheme';
UPDATE implementation set description='GainRatioAttributeEval :

Evaluates the worth of an attribute by measuring the symmetrical uncertainty with respect to the class. 

 SymmU(Class, Attribute) = 2 * (H(Class) - H(Class | Attribute)) / H(Class) + H(Attribute).
' where name='weka.SymmetricalUncertAttributeEval';
UPDATE implementation set description='DECORATE is a meta-learner for building diverse ensembles of classifiers by using specially constructed artificial training examples. Comprehensive experiments have demonstrated that this technique is consistently more accurate than the base classifier, Bagging and Random Forests.Decorate also obtains higher accuracy than Boosting on small training sets, and achieves comparable performance on larger training sets. For more details see: P. Melville & R. J. Mooney. Constructing diverse classifier ensembles using artificial training examples (IJCAI 2003).
P. Melville & R. J. Mooney. Creating diversity in ensembles using artificial data (submitted).' where name='weka.Decorate';
UPDATE implementation set description='SVMAttributeEval :

Evaluates the worth of an attribute by using an SVM classifier.

For more information see:
Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. Machine Learning, 46, 389-422' where name='weka.SVMAttributeEval';
UPDATE implementation set description='Implementation for doing classification using regression methods. Implementation is binarized and one regression model is built for each class value. For more information, see, for example

E. Frank, Y. Wang, S. Inglis, G. Holmes, and I.H. Witten (1998) "Using model trees for classification", Machine Learning, Vol.32, No.1, pp. 63-76.' where name='weka.ClassificationViaRegression';
UPDATE implementation set description='A data generator that produces data points in clusters.' where name='weka.BIRCHCluster';
UPDATE implementation set description='CfsSubsetEval :

Evaluates the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them.

Subsets of features that are highly correlated with the class while having low intercorrelation are preferred.
' where name='weka.CfsSubsetEval';
UPDATE implementation set description='Implementation for generating a pruned or unpruned C4.5 decision tree. For more information, see

Ross Quinlan (1993). "C4.5: Programs for Machine Learning", Morgan Kaufmann Publishers, San Mateo, CA.

' where name='weka.J48';
UPDATE implementation set description='Visualize incoming data/training/test sets in a 2D scatter plot.' where name='weka.DataVisualizer';
UPDATE implementation set description='Reads Instances from a Database' where name='weka.DatabaseLoader';
UPDATE implementation set description='Implements John Platt\'s sequential minimal optimization algorithm for training a support vector classifier.

This implementation globally replaces all missing values and transforms nominal attributes into binary ones. It also normalizes all attributes by default. (In that case the coefficients in the output are based on the normalized data, not the original data --- this is important for interpreting the classifier.)

Multi-class problems are solved using pairwise classification.

To obtain proper probability estimates, use the option that fits logistic regression models to the outputs of the support vector machine. In the multi-class case the predicted probabilities are coupled using Hastie and Tibshirani\'s pairwise coupling method.

Note: for improved speed normalization should be turned off when operating on SparseInstances.

For more information on the SMO algorithm, see

J. Platt (1998). "Fast Training of Support Vector Machines using Sequential Minimal Optimization". Advances in Kernel Methods - Support Vector Learning, B. Schoelkopf, C. Burges, and A. Smola, eds., MIT Press. 

S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, K.R.K. Murthy,  "Improvements to Platt\'s SMO Algorithm for SVM Classifier Design".  Neural Computation, 13(3), pp 637-649, 2001.' where name='weka.SMO';
UPDATE implementation set description='Implements StackingC (more efficient version of stacking). For more information, see

Seewald A.K.: "How to Make Stacking Better and Faster While Also Taking Care of an Unknown Weakness", in Sammut C., Hoffmann A. (eds.), Proceedings of the Nineteenth International Conference on Machine Learning (ICML 2002), Morgan Kaufmann Publishers, pp.554-561, 2002.

Note: requires meta classifier to be a numeric prediction scheme.' where name='weka.StackingC';
UPDATE implementation set description='Tells a sub-ResultProducer to reproduce the current run for varying sized subsamples of the dataset. Normally used with an AveragingResultProducer and CrossValidationResultProducer combo to generate learning curve results.' where name='weka.LearningRateResultProducer';
UPDATE implementation set description='Implementation for building and using a simple Naive Bayes classifier.Numeric attributes are modelled by a normal distribution. For more information, see

Richard Duda and Peter Hart (1973). Pattern Classification and Scene Analysis. Wiley, New York.' where name='weka.NaiveBayesSimple';
UPDATE implementation set description='Implementation for bagging a classifier to reduce variance. Can do classification and regression depending on the base learner. For more information, see

Leo Breiman (1996). "Bagging predictors". Machine Learning, 24(2):123-140.' where name='weka.Bagging';
UPDATE implementation set description='An instance filter that copies a range of attributes in the dataset. This is used in conjunction with other filters that overwrite attribute values during the course of their operation -- this filter allows the original attributes to be kept as well as the new attributes.' where name='weka.Copy';
UPDATE implementation set description='Nearest-neighbor-like algorithm using non-nested generalized exemplars (which are hyperrectangles that can be viewed as if-then rules). For more information, see 

Brent Martin, (1995) "Instance-Based learning : Nearest Neighbor With Generalization", Master Thesis, University of Waikato, Hamilton, New Zealand

Sylvain Roy (2002) "Nearest Neighbor With Generalization",Unpublished, University of Canterbury, Christchurch, New Zealand

' where name='weka.NNge';
UPDATE implementation set description='Writes to a destination that is in the format used by the C4.5 algorithm.
Therefore it outputs a names and a data file.' where name='weka.C45Saver';
UPDATE implementation set description='Takes results from a result producer and assembles them into comma separated value form.' where name='weka.CSVResultListener';
UPDATE implementation set description='The original algorithm M5 was invented by Quinlan:
Quinlan J. R. (1992). Learning with continuous classes. Proceedings of the Australian Joint Conference on Artificial Intelligence. 343--348. World Scientific, Singapore.

Yong Wang made improvements and created M5\':
Wang, Y and Witten, I. H. (1997). Induction of model trees for predicting continuous classes. Proceedings of the poster papers of the European Conference on Machine Learning. University of Economics, Faculty of Informatics and Statistics, Prague.' where name='weka.M5P';
UPDATE implementation set description='Removes attributes of a given type.' where name='weka.RemoveType';
UPDATE implementation set description='Performs a cross validation run using a supplied evaluator.' where name='weka.CrossValidationResultProducer';
UPDATE implementation set description='Merges two values of a nominal attribute into one value.' where name='weka.MergeTwoValues';
